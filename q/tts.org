#+date: Nov 2024 

#+latex_class: tiet-question-paper
#+latex_class_options: [12pt]
#+options: num:nil toc:nil author:nil email:nil 

#+latex_header_extra: \hypersetup{%
#+latex_header_extra:   colorlinks,%
#+latex_header_extra:   breaklinks,%
#+latex_header_extra:   urlcolor=[rgb]{0,0.35,0.65},%
#+latex_header_extra:   linkcolor=[rgb]{0,0.35,0.65}%
#+latex_header_extra: }

#+latex_header_extra: \usepackage{libertinus}

#+latex_header_extra: \instlogo{images/tiet-logo.pdf}
#+latex_header_extra: \schoolordepartment{%
#+latex_header_extra: Computer Science \& Engineering Department}
#+latex_header_extra: \examname{Exercise (TTS) (2024-25 Odd)}
#+latex_header_extra: \coursecode{UCS749}
#+latex_header_extra: \coursename{Conversational AI: Speech Proc. […]}
#+latex_header_extra: \timeduration{2 hours}
#+latex_header_extra: \maxmarks{--}
#+latex_header_extra: \faculty{RGB}

#+latex: \thispagestyle{empty}
#+latex: \maketitle

\bvrskipline[-1.85]
\bvrhrule

1. If $P_\theta$ models a data $\mathcal{D}$ with an
   intent of generating novel samples, $\mathbf{x}\sim
   P_\theta$.  Is $P_\theta$ a posterior distribution?
   Comment. \hfill [2 marks]

\bvrskipline[0.25]
#+attr_latex: :options [resume]
1. If $X\equiv\{\mathbf{x}_1,\ldots,\mathbf{x}_T\}$ is
   a temporal sequence and each time step is
   independently and identically distributed,
   1. Prove that $\log P(X) = \sum_{i=1}^T\log P(x_i)$;
   2. If $X$ is a Bernoulli process, /i.e./ if each
      $x_{i}$ is a coin toss, with hit rate $p$, the
      log likelihood of $k$ hits is $k\log p +
      (T-k)\log (1-p)$.



#+DOWNLOADED: screenshot @ 2024-11-12 10:15:18
[[file:org-download-images/2024-11-12_10-15-18_screenshot.png]]

#+DOWNLOADED: screenshot @ 2024-11-12 10:18:39
[[file:org-download-images/2024-11-12_10-18-39_screenshot.png]]


\bvrskipline[0.25]
#+attr_latex: :options [resume]
1. If $X\equiv\{\mathbf{x}_1,\ldots,\mathbf{x}_T\}$ is
   the temporal sequence of a Markov process, prove
   that $\log P(X) = \sum_{i=1}^T \log P(x_i\mid x_{i-1})$.


\newpage
\bvrskipline[0.25]
#+attr_latex: :options [resume]

#+latex: {\huge

# $X\equiv\{x_1,\ldots,x_T:x_i\in\mathbb{R}\}$ \\
# At each time step $i$, the value of intensity $x_{i}$
# is implicitly continuous.  Hence, $x_{i}\in\mathbb{R}$.

# But it may as well be quantised into $N$ values, so
# that, $x_{i}\in\mathbb{Z}_{[0,N]}$

# $X\equiv\left\{ x_1,\ldots,x_T:x_i\in\mathbb{Z}_{[0,N]}
#   \right\}$

# The advantage of using discrete values, is that each
# $x_{i}$ may now be modelled as a categorical
# distribution. 

Speech sample $X$ is a temporal sequence of intensities,

\begin{align*}
  X&\equiv\{x_1\ldots x_T\}
\end{align*}

Given speech samples $\mathcal{D}$ as evidence,
estimate $P_{\theta}\approx \mathcal{D}$ so that $X\sim
P_\theta$ is a valid speech sample.

#+latex: }

\newpage
\bvrhrule

** Unconditional Generation                                         :ignore:
#+latex: {\huge

Let $G_{\theta,\mathcal{N}} : \mathbb{X}^K \to
\mathbb{X}$ represent the model.

where,
+ $\mathbb{X}$ is the field of inputs.
  + For a continuous model, $\mathbb{X}\equiv
    \mathbb{R}$; whereas,
  + For categorical model,
    $\mathbb{X}\equiv\mathbb{R}_{[0,1]}^{256}$;
  + It may also be a hybrid model, \\
    /e.g./ $G_{\theta,\mathcal{N}} : \mathbb{R}^K \to
    \mathbb{R}_{[0,1]}^{256}$. \\
    Can you think how?

Let $G_{\theta,\mathcal{N}} : \mathbb{X}^K \to
\mathbb{X}$ represent the model.

where,
+ $\mathcal{N}$ is a noise sampler,
  + Generally, implemented as a normal distribution; or
  + Implemented implicitly as dropouts.

Let $G_{\theta,\mathcal{N}} : \mathbb{X}^K \to
\mathbb{X}$ represent the model.

where,
+ $\theta$ are parameters of the model; and
+ $\mathbf{x}_{t} = G_{\theta,\mathcal{N}}
  \left( \begin{bmatrix} \mathbf{x}_{t-K} & \ldots &
  \mathbf{x}_{t-1} \end{bmatrix} \right)$ models the
  conditional distribution $P(x_t \mid x_{t-K}\ldots
  x_{t-1})$

\newpage

In case of Conditional Generation,

$\mathbf{x}_{t} = G_{\theta,\mathcal{N}}
\left( \begin{bmatrix} \mathbf{x}_{t-K} & \ldots &
\mathbf{x}_{t-1} \end{bmatrix}, \mathbf{h} \right)$ \\
models the conditional distribution \\
$P(x_t \mid x_{t-K}\ldots x_{t-1}, \mathbf{h})$

$\mathbf{h}$ represents the global conditions, /e.g./
+ Text input;
+ Speaker;
+ Accent;
+ and so forth.

\newpage

Let \\
$X\equiv\begin{bmatrix} \mathbf{x}_1 & \ldots &
\mathbf{x}_T \end{bmatrix} \sim G_{\theta,\mathcal{N}}$
be the output of the auto-regressive model, so that \\
$\mathbf{x}_{t} = G_{\theta,\mathcal{N}}
\left( \begin{bmatrix} \mathbf{x}_{t-K} & \ldots &
\mathbf{x}_{t-1} \end{bmatrix} \right)$

\bvrskipline[0.25]

And, \\
$Y\equiv\begin{bmatrix} \mathbf{y}_1 & \ldots &
\mathbf{y}_T \end{bmatrix} \sim \mathcal{D}$ be a
speech sample from dataset.  Recall that for each time
step, the sound intensity is pre-processed so that the
values are remapped, quantised, and converted to
one-hot vectors, so that $\mathbf{y}_t\in\{0,1\}^{256};
\|\mathbf{y}_t\|_{1}=1$ .
 
\bvrskipline[0.25]

The training objective is the cross entropy function,
given as,
\begin{align*}
  \underset{G}{\text{minimise}}\quad
  \underset{X\sim G, Y\sim\mathcal{D}}{\mathbb{E}}
  \left[ \sum_{i,t} y_{i,t}\log x_{i,t} \right]
\end{align*}


\newpage

*Tacotron*

*Artificial Neuron*

\begin{align*}
  \mathbf{y} &= \mathcal{N}(W\mathbf{x} + \mathbf{b})
\end{align*}

where,
+ $\mathbf{x},\mathbf{y}\in V$, for some vector space
  $V$;
+ $W,\mathbf{b}$ are learnable weights; and
+ $\mathcal{N}$ is a non-linearity applied
  element-wise.

without loss of generality \\
*ANN Layer*

\begin{align*}
  \mathbf{x}_{l+1} &= \mathcal{N}(W_l\mathbf{x}_l +
                     \mathbf{b}_l) 
\end{align*}

where,
+ $\mathbf{x}_{l}\in V \;\forall\, l$, for some vector space
  $V$;
+ $W,\mathbf{b}$ are learnable weights; and
+ $\mathcal{N}$ is a non-linearity applied
  element-wise.

*Sequential Network* \\
e.g. AlexNet, VGG Net etc.

\newpage

\begin{align*}
  \mathbf{y} &=\mathbf{g} \otimes \mathbf{x} \\
  \mathbf{g} &= \sigma(W\mathbf{x}+\mathbf{b})
\end{align*}
where,
+ $\otimes$ represents Hadamard product;
+ $\sigma$ represents the logistic sigmoid function
  that is applied element-wise; and
+ $\mathbf{g}\in\mathbb{R}^n_{[0,1]}$ represents the
  gate.


*Highway Gate*
\begin{align*}
  \mathbf{y} &= (\boldsymbol{1} - \mathbf{g}) \otimes
               \mathbf{x} + \mathbf{g} \otimes
               \mathbf{h} \\
  \mathbf{g} &= W_g\mathbf{x}+\mathbf{b}_g \\
  \mathbf{h} &= W_h\mathbf{x}+\mathbf{b}_h
\end{align*}


*Highway Gate*

In practice,
\begin{align*}
  \mathbf{y} &= (\boldsymbol{1} - \mathbf{g}) \otimes
               \mathbf{x} + \mathbf{g} \otimes
               \mathbf{h} \\
  (\mathbf{g},\mathbf{h})
             &= \left(\sigma
               (\widetilde{\mathbf{y}}_{:n}),
               \mathcal{N}
               (\widetilde{\mathbf{y}}_{n:}) \right) \\
  \widetilde{\mathbf{y}} &= W\mathbf{x}+\mathbf{b} 
\end{align*}

\newpage
*Constructing a highway network*

$\forall\,l\in\{1,\ldots,L\}$
\begin{align*}
  \mathbf{x}_{l} &= (\boldsymbol{1} - \mathbf{g}_l) \otimes
               \mathbf{x}_l + \mathbf{g}_l \otimes
               \mathbf{h}_l \\
  (\mathbf{g}_l,\mathbf{h}_l)
             &= \left(\sigma
               (\widetilde{\mathbf{x}}_{l,:n}),
               \mathcal{N}
               (\widetilde{\mathbf{x}}_{l,n:}) \right) \\
  \widetilde{\mathbf{x}}_l &= W_l\mathbf{x}_{l-1}+\mathbf{b}_l
\end{align*}

With $L=50$ layer deep models, the very-deep network
building strategy shown here predates googlenet and
resnet.


\begin{align*}
P_\theta\left(Y|X\right)\quad\text{modelled as}\quad
  \mathbf{y} = G(\mathbf{x};\theta)
\end{align*}

as simple \\
as possible

\begin{align*}
  P(\text{speech}\mid\text{text})
\end{align*}

\begin{align*}
  X\equiv\{\mathbf{x}_1\ldots\mathbf{x}_N\} \\
  Y\equiv\{\mathbf{y}_1\ldots\mathbf{y}_N\}
\end{align*}

\begin{align*}
P(x_i\mid x_1,…,x_{i-1},x_{i+1},...,x_T)
\end{align*}

#+latex: }

#+attr_latex: :align |c|c|c|c|c|c|c|c|c|c|c|
|------+------+------+------+------+------+------+------+------+------+------|
| 0.53 | 0.24 | 0.01 | 0.45 | 0.14 | 0.42 | 0.59 | 0.37 | 0.32 | 0.18 | 0.29 |
|------+------+------+------+------+------+------+------+------+------+------|
| 0.56 | 0.16 | 0.32 | 0.44 | 0.32 | 0.63 | 0.52 | 0.85 | 0.75 | 0.75 | 0.33 |
|------+------+------+------+------+------+------+------+------+------+------|
| 0.84 | 0.93 | 0.13 | 0.63 | 0.06 | 0.83 | 0.13 | 0.65 | 0.11 | 0.65 | 0.17 |
|------+------+------+------+------+------+------+------+------+------+------|
| 0.92 | 0.62 | 0.08 | 0.13 | 0.18 | 0.72 | 0.83 | 0.54 | 0.83 | 0.29 | 0.45 |
|------+------+------+------+------+------+------+------+------+------+------|
| 0.41 | 0.33 | 0.89 | 0.71 | 0.33 | 0.86 | 0.15 | 0.13 | 0.68 | 0.99 | 0.14 |
|------+------+------+------+------+------+------+------+------+------+------|
| 0.26 | 0.17 | 0.03 | 0.48 | 0.29 | 0.40 | 0.60 | 0.27 | 0.85 | 0.66 | 0.11 |
|------+------+------+------+------+------+------+------+------+------+------|
| 0.95 | 0.07 | 0.17 | 0.79 | 0.59 | 0.41 | 0.07 | 0.22 | 0.60 | 0.11 | 0.11 |
|------+------+------+------+------+------+------+------+------+------+------|
| 0.27 | 0.72 | 0.02 | 0.97 | 0.62 | 0.53 | 0.49 | 0.81 | 0.00 | 0.29 | 0.52 |
|------+------+------+------+------+------+------+------+------+------+------|
| 0.88 | 0.17 | 0.24 | 0.77 | 0.59 | 0.40 | 0.71 | 0.88 | 0.07 | 0.21 | 0.17 |
|------+------+------+------+------+------+------+------+------+------+------|
| 0.23 | 0.06 | 0.48 | 0.97 | 0.18 | 0.85 | 0.62 | 0.82 | 0.73 | 0.66 | 0.70 |
|------+------+------+------+------+------+------+------+------+------+------|
| 0.67 | 0.66 | 0.24 | 0.46 | 0.19 | 0.87 | 0.83 | 0.19 | 0.15 | 0.21 | 0.70 |
|------+------+------+------+------+------+------+------+------+------+------|
| 0.28 | 0.72 | 0.32 | 0.04 | 0.48 | 0.16 | 0.47 | 0.34 | 0.52 | 0.14 | 0.75 |
|------+------+------+------+------+------+------+------+------+------+------|
| 0.94 | 0.73 | 0.20 | 0.57 | 0.68 | 0.47 | 0.90 | 0.54 | 0.67 | 0.18 | 0.84 |
|------+------+------+------+------+------+------+------+------+------+------|

\vfill
\bvrhrule
\bvrskipline[-0.85]
\bvrhrule
